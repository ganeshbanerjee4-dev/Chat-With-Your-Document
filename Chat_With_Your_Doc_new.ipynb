{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community --upgrade\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install gradio\n",
        "!pip install pandas\n",
        "!pip install -U langchain-huggingface\n",
        "!pip install -U langchain-openai\n",
        "!pip install numpy\n",
        "!pip install openai\n",
        "\n",
        "import gradio as gr\n",
        "from PyPDF2 import PdfReader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Initialize embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize language model\n",
        "os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY", # Users: replace or set as environment variable before running
        "llm = ChatOpenAI(\n",
        "    model=\"openai/gpt-oss-20b:free\",\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    temperature=0.5,\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# Global variables for FAISS\n",
        "vectorstore = None\n",
        "retriever = None\n",
        "\n",
        "def process_file(uploaded_file_path):\n",
        "    \"\"\"Processes the uploaded file and updates the knowledge base.\"\"\"\n",
        "    global vectorstore, retriever\n",
        "\n",
        "    text = \"\"\n",
        "\n",
        "    # Read document based on file type\n",
        "    if uploaded_file_path.endswith(\".txt\"):\n",
        "        with open(uploaded_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text = file.read()\n",
        "\n",
        "    elif uploaded_file_path.endswith(\".pdf\"):\n",
        "        reader = PdfReader(uploaded_file_path)\n",
        "        text = \"\\n\".join([page.extract_text() or \"\" for page in reader.pages]).strip()\n",
        "\n",
        "    elif uploaded_file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(uploaded_file_path)\n",
        "        text = df.to_string()\n",
        "\n",
        "    else:\n",
        "        return \"Unsupported file format.\"\n",
        "\n",
        "    if not text:\n",
        "        return \"Error: No readable text found in the file.\"\n",
        "\n",
        "    # Text chunking using RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=750,\n",
        "        chunk_overlap=100,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "    document_texts = text_splitter.split_text(text)\n",
        "\n",
        "    # Create Document objects with metadata\n",
        "    documents = [\n",
        "        Document(page_content=chunk, metadata={\"source\": f\"chunk_{i}\"})\n",
        "        for i, chunk in enumerate(document_texts)\n",
        "    ]\n",
        "\n",
        "    # Create FAISS vectorstore from documents with metadata\n",
        "    vectorstore = FAISS.from_documents(documents, embedding_model)\n",
        "\n",
        "    # Create retriever\n",
        "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
        "\n",
        "    return f\"File processed successfully! {len(document_texts)} chunks added to FAISS.\"\n",
        "\n",
        "def retrieve_relevant_text(query):\n",
        "    \"\"\"Retrieves top matching document chunks from LangChain's FAISS retriever.\"\"\"\n",
        "    if retriever is None:\n",
        "        return \"\"\n",
        "\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "    # Optional: You can use metadata here (e.g., show chunk source)\n",
        "    context = \"\\n\\n\".join(\n",
        "        [f\"[{doc.metadata['source']}]: {doc.page_content}\" for doc in docs]\n",
        "    )\n",
        "\n",
        "    return context\n",
        "\n",
        "def chat_with_document(message, history):\n",
        "    \"\"\"Chat function that integrates document retrieval and OpenRouter LLM (gpt-oss-20b).\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Retrieve relevant text from FAISS\n",
        "        context = retrieve_relevant_text(message)\n",
        "\n",
        "        if not context.strip():\n",
        "            yield \"I could not find that information in the provided document.\"\n",
        "            return\n",
        "\n",
        "        # System prompt with strict instruction\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"\"\"You are a helpful assistant. You must answer only using the information in the provided document context.\n",
        "If the answer is not explicitly found in the document, say \"I could not find that information in the provided document.\"\n",
        "Strictly avoid using any external knowledge or making assumptions.\n",
        "\n",
        "Document Context:\n",
        "{context}\n",
        "\"\"\"}\n",
        "        ]\n",
        "\n",
        "        # Add past messages from history (dict-based now)\n",
        "        for msg in history:\n",
        "            messages.append(msg)\n",
        "\n",
        "        # Add current user message\n",
        "        messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "        # Generate response using Groq's Llama3 model\n",
        "        response = llm.invoke(messages)\n",
        "\n",
        "        yield {\"role\": \"assistant\", \"content\": response.content}\n",
        "\n",
        "    except Exception as e:\n",
        "        yield {\"role\": \"assistant\", \"content\": f\"Error occurred: {str(e)}\"}\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Document-Aware Chatbot\")\n",
        "\n",
        "    with gr.Row():\n",
        "        upload_button = gr.File(label=\"Upload a file (TXT, PDF, CSV)\", type=\"filepath\")\n",
        "        process_output = gr.Textbox(label=\"Processing Status\", interactive=False)\n",
        "\n",
        "    upload_button.change(process_file, inputs=upload_button, outputs=process_output)\n",
        "\n",
        "    chat_interface = gr.ChatInterface(\n",
        "        fn=chat_with_document,\n",
        "        title=\"Chat with Your Document\",\n",
        "        type=\"messages\"\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "id": "F97ItdcmQzev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
